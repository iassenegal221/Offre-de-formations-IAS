{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "61eed29c",
      "metadata": {
        "id": "61eed29c"
      },
      "source": [
        "# Deep Learning IAS \n",
        "## Auteur: Papa Séga WADE [https://solo.to/mathspsw]\n",
        "_________________________________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e71adc26",
      "metadata": {
        "id": "e71adc26"
      },
      "source": [
        "# Exemple d'application du chapitre 1: images MNIST\n",
        "##### On va faire de la reconnaissance de l'écriture manuscrite ! Comme le font les ipad pro ou galaxy note ;)) Soo funny\n",
        "\n",
        "Dans cet exemple nous allons voir comment importer un jeu (ou le jeu de données MNIST) depuis la bibliothèque **Keras** developpée par François Chollet chercheur chez Google et l'auteur du livre **Deep Learning with Python**.\n",
        "\n",
        "Ce jeu de données est composé de 60.000 images pour l'aprrentissage du modèle et 10.000 images pour le test sur les 10 labels des images manuscrites de **0 à 9**. \n",
        "\n",
        "##### MNIST \n",
        "Pour dire Mixted or Modified National Institut of Standford Technology c'est une base de données de chiffres écrit à la main. Ce jeu de données est un test stardard pour la reconnaissance d'écriture manuscrit qui est problème difficile. \n",
        "\n",
        "Pour en savoir beaucoup plus, voici un lien que vous pouvez lire de Yann Lecun dont j'ai évoqué dans le cours http://yann.lecun.com/exdb/mnist/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb69de1c",
      "metadata": {
        "id": "cb69de1c"
      },
      "outputs": [],
      "source": [
        "## ici cette ligne nous permet d'importer le jeu de données mnist depuis keras avec les deux mots clés \"from ... import\"\n",
        "from tensorflow.keras.datasets import mnist  \n",
        "## la deuxième ligne nous permet de charger le jeux de données en le scindant en données d'apprentissage et de test avec leurs labels \n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3ec9bf4",
      "metadata": {
        "id": "d3ec9bf4",
        "outputId": "f65159cc-650d-4441-ed79-93a5c5624a90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# cette ligne de code nous permet de savoir la dimension de notre jeu de données avec la fonction \"nom_jeu_de_données.shape\"\n",
        "train_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "659f3b42-62c7-43db-b68d-5f079376cc94",
      "metadata": {
        "id": "659f3b42-62c7-43db-b68d-5f079376cc94",
        "outputId": "5bf9632c-a2ce-4562-89df-37935176419b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## idem pour savoir le nombre de données test\n",
        "test_images.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af535ca5",
      "metadata": {
        "id": "af535ca5"
      },
      "source": [
        "A présent, on connait le nombre et la taille des images. Nous avons des images en noir et blanc, normalisées et centrées de 28 pixels sur chaque côté (des images carrées) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eb80aaa",
      "metadata": {
        "id": "3eb80aaa"
      },
      "source": [
        "### Construction de notre modèle "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5197d68",
      "metadata": {
        "id": "f5197d68"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras  #importation de keras depuis tensorflow \n",
        "from tensorflow.keras import layers #fontion layers permettant de definir les différentes couches de notre réseau de neurones\n",
        "# Ces lignes suivantes nous permettent de définir le modèle d'apprentissage qui n'est rien d'autres qu'un modèle sequentiel \n",
        "# Un modèle sequential est approprié pour une simple pile de couches où chaque couche a exactement un tenseur d'entrée et un tenseur de sortie .\n",
        "model_IAS1 = keras.Sequential([\n",
        "layers.Dense(512, activation=\"relu\"),     ## ici nous avons une couche de 512 neurones avec une fontion d'activation ReLu\n",
        "layers.Dense(10, activation=\"softmax\")    ## cette partie de notre modèle est la couche de sortie avec 10 neurones\n",
        "                                           ##  ces 10 neurones ne sont rien d'autre que le nombre de classe d'images à reconnaître\n",
        "                                           ##  avec la fonction d'activation Softmax \n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fcd5afd",
      "metadata": {
        "id": "1fcd5afd"
      },
      "source": [
        "## Compilation du modele_IAS1\n",
        "Pour compiler le modèle nous avons besoin premièrement, d'une fonction d'optimisation ici **rsmprop** et le rôle de RMSprop est de :\n",
        "\n",
        "    -Maintenir une moyenne mobile (actualisée) du carré des gradients\n",
        "    -Diviser le gradient par la racine de cette moyenne\n",
        "\n",
        "Ensuite une fonction loss **sparse_categorical_crossentropy** qui calcule la perte d'entropie croisée entre les vraies étiquettes et celles prédites.\n",
        "Et enfin une métrique **accuracy** qui calcule la fréquence à laquelle les prédictions correspondent aux labels. Sa valeur est comprise entre 0 et 1 (ou entre 0 et 100%). \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f7dc99e",
      "metadata": {
        "id": "5f7dc99e"
      },
      "outputs": [],
      "source": [
        "model_IAS1.compile(optimizer=\"rmsprop\",\n",
        "loss=\"sparse_categorical_crossentropy\",\n",
        "metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f5a8bec",
      "metadata": {
        "id": "7f5a8bec"
      },
      "outputs": [],
      "source": [
        "## Cette partie est la phase de normlisation des images afin qu'elles puissent être exploitables par l'ordinateur \n",
        "# car initialement nos images d'entraînement étaient stockées dans un tableau de forme (60000, 28, 28) de type uint8 avec des valeurs dans l'intervalle [0, 255].\n",
        "# Nous allons les transformer en un tableau float32 de forme (60000, 28 *28) avec des valeurs comprises entre 0 et 1.\n",
        "\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype(\"float32\") / 255"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed68b937",
      "metadata": {
        "id": "ed68b937"
      },
      "source": [
        "________________________________________________________________________________________________________________________\n",
        "Ici on va *fiter* le modèle sur **5 epochs** c'est-à-dire 5 passages dans les données d'apprentissage et un **batch_size de 128** c'est-à-dire les données d'entrée sont par paquet de 128. \n",
        "\n",
        "/!\\ Et si vous remerquez bien ici, nous n'avons pas utilisé les données de test !\n",
        "\n",
        "\n",
        "**Explication:**\n",
        "\n",
        "Intuitivement pour évaluer le niveau des élèves dans une classe, le professeur ne donne pas à la fois les exercices et le sujet d'examen. Ici **les exercices** sont nos **données d'entraînement** et **le sujet d'examen** est **les données de test**.\n",
        "\n",
        "Au cas où le professeur donne les élèves à la fois *les exercieces* et *le sujet d'examen* avec la correction, l'estimation du niveau des élèves (contrôle de connaissances) sera fortement biaisée du fait que tous les éléves auront la même note, vous serez sans doute d'accord avec moi ! \n",
        "\n",
        "C'est ce qui explique que dans l'apprentissage on ne peut pas utiliser à la fois les données d'entrînement et les données de test, sinon notre modèle de reconnaissance sera fortement biaisé et ne fera que recopier ce qu'il a appris. \n",
        "_________________________________________________________________________________________________________________________\n",
        "\n",
        "Passons maintenant à l'entraînement de notre modèle de reconnaissance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f55c063c",
      "metadata": {
        "id": "f55c063c"
      },
      "outputs": [],
      "source": [
        "model_IAS1.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30e92483",
      "metadata": {
        "id": "30e92483"
      },
      "outputs": [],
      "source": [
        "test_digits = test_images[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afa1a713",
      "metadata": {
        "id": "afa1a713",
        "outputId": "2ba3cd86-fda9-49fa-83b7-2ff4968fef4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_digitsf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20ebe647",
      "metadata": {
        "id": "20ebe647"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(test_digits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0b47b02",
      "metadata": {
        "id": "d0b47b02",
        "outputId": "0cea8cc5-4852-49d1-bc9a-339f254e1437"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1.4323790e-08, 1.0568703e-10, 8.5563370e-06, 5.2710875e-05,\n",
              "       1.5031945e-12, 1.4452939e-08, 7.4887301e-15, 9.9993718e-01,\n",
              "       8.4092795e-07, 7.0164396e-07], dtype=float32)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "188809fd",
      "metadata": {
        "id": "188809fd",
        "outputId": "7a816440-a98a-4b2b-8967-42e85e534b20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions[0].argmax()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b06de9f1",
      "metadata": {
        "id": "b06de9f1",
        "outputId": "ccbd21cf-0871-461a-9beb-6a7e47fcaf8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9999372"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions[0][7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4c98e04",
      "metadata": {
        "id": "b4c98e04",
        "outputId": "fdb31c23-88c6-44d4-c0db-765e4d9dbc62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_labels[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16bcc89e",
      "metadata": {
        "id": "16bcc89e",
        "outputId": "4466884f-bb8a-4e66-c1e5-6d4bae3e6c71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 0s 895us/step - loss: 0.0653 - accuracy: 0.9797\n",
            "test_acc: 0.9797000288963318\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"test_acc: {test_acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Affichage des images "
      ],
      "metadata": {
        "id": "caCKM3r6Otcr"
      },
      "id": "caCKM3r6Otcr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31ec4cac",
      "metadata": {
        "id": "31ec4cac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ec9c48b-a98b-4787-c486-24e77ee5deef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# On pourrait ne pas charger les données de nouveau !\n",
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f1a00d6",
      "metadata": {
        "id": "6f1a00d6",
        "outputId": "267fb5a0-7e65-424a-94a9-aaedfd16102a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOUElEQVR4nO3dX4xUdZrG8ecF8R+DCkuHtAyRGTQmHY1AStgEg+hk8U+iwI2BGERjxAuQmQTiolzAhRdGd2YyihnTqAE2IxPCSITErIMEY4iJoVC2BZVFTeNA+FOE6Dh6gTLvXvRh0mLXr5qqU3XKfr+fpNPV56nT502Fh1Ndp7t+5u4CMPQNK3oAAK1B2YEgKDsQBGUHgqDsQBAXtfJgY8eO9YkTJ7bykEAovb29OnXqlA2UNVR2M7tT0h8kDZf0krs/nbr/xIkTVS6XGzkkgIRSqVQ1q/tpvJkNl/SCpLskdUlaYGZd9X4/AM3VyM/s0yR96u6fu/sZSX+WNCefsQDkrZGyj5f0t35fH8m2/YCZLTazspmVK5VKA4cD0Iimvxrv7t3uXnL3UkdHR7MPB6CKRsp+VNKEfl//PNsGoA01UvY9kq4zs1+Y2cWS5kvals9YAPJW96U3d//ezJZKelN9l95ecfcDuU0GIFcNXWd39zckvZHTLACaiF+XBYKg7EAQlB0IgrIDQVB2IAjKDgRB2YEgKDsQBGUHgqDsQBCUHQiCsgNBUHYgCMoOBEHZgSAoOxAEZQeCoOxAEJQdCIKyA0FQdiAIyg4EQdmBICg7EARlB4Kg7EAQlB0IgrIDQVB2IIiGVnFF+zt79mwy/+qrr5p6/LVr11bNvv322+S+Bw8eTOYvvPBCMl+xYkXVbNOmTcl9L7300mS+cuXKZL569epkXoSGym5mvZK+lnRW0vfuXspjKAD5y+PMfpu7n8rh+wBoIn5mB4JotOwu6a9mttfMFg90BzNbbGZlMytXKpUGDwegXo2W/RZ3nyrpLklLzGzm+Xdw9253L7l7qaOjo8HDAahXQ2V396PZ55OStkqalsdQAPJXd9nNbKSZjTp3W9JsSfvzGgxAvhp5NX6cpK1mdu77vOru/5PLVEPMF198kczPnDmTzN99991kvnv37qrZl19+mdx3y5YtybxIEyZMSOaPPfZYMt+6dWvVbNSoUcl9b7rppmR+6623JvN2VHfZ3f1zSelHBEDb4NIbEARlB4Kg7EAQlB0IgrIDQfAnrjn44IMPkvntt9+ezJv9Z6btavjw4cn8qaeeSuYjR45M5vfff3/V7Oqrr07uO3r06GR+/fXXJ/N2xJkdCIKyA0FQdiAIyg4EQdmBICg7EARlB4LgOnsOrrnmmmQ+duzYZN7O19mnT5+ezGtdj961a1fV7OKLL07uu3DhwmSOC8OZHQiCsgNBUHYgCMoOBEHZgSAoOxAEZQeC4Dp7DsaMGZPMn3322WS+ffv2ZD5lypRkvmzZsmSeMnny5GT+1ltvJfNaf1O+f3/1pQSee+655L7IF2d2IAjKDgRB2YEgKDsQBGUHgqDsQBCUHQiC6+wtMHfu3GRe633lay0v3NPTUzV76aWXkvuuWLEimde6jl7LDTfcUDXr7u5u6HvjwtQ8s5vZK2Z20sz299s2xsx2mNmh7HP6HQwAFG4wT+PXS7rzvG0rJe109+sk7cy+BtDGapbd3d+RdPq8zXMkbchub5CUfp4KoHD1vkA3zt2PZbePSxpX7Y5mttjMymZWrlQqdR4OQKMafjXe3V2SJ/Judy+5e6mjo6PRwwGoU71lP2FmnZKUfT6Z30gAmqHesm+TtCi7vUjS6/mMA6BZal5nN7NNkmZJGmtmRyStlvS0pM1m9rCkw5Lua+aQQ90VV1zR0P5XXnll3fvWug4/f/78ZD5sGL+X9VNRs+zuvqBK9KucZwHQRPy3DARB2YEgKDsQBGUHgqDsQBD8iesQsGbNmqrZ3r17k/u+/fbbybzWW0nPnj07maN9cGYHgqDsQBCUHQiCsgNBUHYgCMoOBEHZgSC4zj4EpN7ued26dcl9p06dmswfeeSRZH7bbbcl81KpVDVbsmRJcl8zS+a4MJzZgSAoOxAEZQeCoOxAEJQdCIKyA0FQdiAIrrMPcZMmTUrm69evT+YPPfRQMt+4cWPd+TfffJPc94EHHkjmnZ2dyRw/xJkdCIKyA0FQdiAIyg4EQdmBICg7EARlB4LgOntw8+bNS+bXXnttMl++fHkyT73v/BNPPJHc9/Dhw8l81apVyXz8+PHJPJqaZ3Yze8XMTprZ/n7b1pjZUTPbl33c3dwxATRqME/j10u6c4Dtv3f3ydnHG/mOBSBvNcvu7u9IOt2CWQA0USMv0C01s57saf7oancys8VmVjazcqVSaeBwABpRb9n/KGmSpMmSjkn6bbU7unu3u5fcvdTR0VHn4QA0qq6yu/sJdz/r7v+UtE7StHzHApC3uspuZv3/tnCepP3V7gugPdS8zm5mmyTNkjTWzI5IWi1plplNluSSeiU92sQZUaAbb7wxmW/evDmZb9++vWr24IMPJvd98cUXk/mhQ4eS+Y4dO5J5NDXL7u4LBtj8chNmAdBE/LosEARlB4Kg7EAQlB0IgrIDQZi7t+xgpVLJy+Vyy46H9nbJJZck8++++y6ZjxgxIpm/+eabVbNZs2Yl9/2pKpVKKpfLA651zZkdCIKyA0FQdiAIyg4EQdmBICg7EARlB4LgraSR1NPTk8y3bNmSzPfs2VM1q3UdvZaurq5kPnPmzIa+/1DDmR0IgrIDQVB2IAjKDgRB2YEgKDsQBGUHguA6+xB38ODBZP78888n89deey2ZHz9+/IJnGqyLLkr/8+zs7Ezmw4ZxLuuPRwMIgrIDQVB2IAjKDgRB2YEgKDsQBGUHguA6+09ArWvZr776atVs7dq1yX17e3vrGSkXN998czJftWpVMr/33nvzHGfIq3lmN7MJZrbLzD4yswNm9uts+xgz22Fmh7LPo5s/LoB6DeZp/PeSlrt7l6R/l7TEzLokrZS0092vk7Qz+xpAm6pZdnc/5u7vZ7e/lvSxpPGS5kjakN1tg6S5zRoSQOMu6AU6M5soaYqk9ySNc/djWXRc0rgq+yw2s7KZlSuVSgOjAmjEoMtuZj+T9BdJv3H3v/fPvG91yAFXiHT3bncvuXupo6OjoWEB1G9QZTezEeor+p/c/dyfQZ0ws84s75R0sjkjAshDzUtvZmaSXpb0sbv/rl+0TdIiSU9nn19vyoRDwIkTJ5L5gQMHkvnSpUuT+SeffHLBM+Vl+vTpyfzxxx+vms2ZMye5L3+imq/BXGefIWmhpA/NbF+27Un1lXyzmT0s6bCk+5ozIoA81Cy7u++WNODi7pJ+le84AJqF50lAEJQdCIKyA0FQdiAIyg4EwZ+4DtLp06erZo8++mhy33379iXzzz77rK6Z8jBjxoxkvnz58mR+xx13JPPLLrvsgmdCc3BmB4Kg7EAQlB0IgrIDQVB2IAjKDgRB2YEgwlxnf++995L5M888k8z37NlTNTty5EhdM+Xl8ssvr5otW7YsuW+tt2seOXJkXTOh/XBmB4Kg7EAQlB0IgrIDQVB2IAjKDgRB2YEgwlxn37p1a0N5I7q6upL5Pffck8yHDx+ezFesWFE1u+qqq5L7Ig7O7EAQlB0IgrIDQVB2IAjKDgRB2YEgKDsQhLl7+g5mEyRtlDROkkvqdvc/mNkaSY9IqmR3fdLd30h9r1Kp5OVyueGhAQysVCqpXC4PuOryYH6p5ntJy939fTMbJWmvme3Ist+7+3/lNSiA5hnM+uzHJB3Lbn9tZh9LGt/swQDk64J+ZjeziZKmSDr3Hk9LzazHzF4xs9FV9llsZmUzK1cqlYHuAqAFBl12M/uZpL9I+o27/13SHyVNkjRZfWf+3w60n7t3u3vJ3UsdHR05jAygHoMqu5mNUF/R/+Tur0mSu59w97Pu/k9J6yRNa96YABpVs+xmZpJelvSxu/+u3/bOfnebJ2l//uMByMtgXo2fIWmhpA/N7Nzaw09KWmBmk9V3Oa5XUnrdYgCFGsyr8bslDXTdLnlNHUB74TfogCAoOxAEZQeCoOxAEJQdCIKyA0FQdiAIyg4EQdmBICg7EARlB4Kg7EAQlB0IgrIDQdR8K+lcD2ZWkXS436axkk61bIAL066ztetcErPVK8/ZrnH3Ad//raVl/9HBzcruXipsgIR2na1d55KYrV6tmo2n8UAQlB0Iouiydxd8/JR2na1d55KYrV4tma3Qn9kBtE7RZ3YALULZgSAKKbuZ3WlmB83sUzNbWcQM1ZhZr5l9aGb7zKzQ9aWzNfROmtn+ftvGmNkOMzuUfR5wjb2CZltjZkezx26fmd1d0GwTzGyXmX1kZgfM7NfZ9kIfu8RcLXncWv4zu5kNl/R/kv5D0hFJeyQtcPePWjpIFWbWK6nk7oX/AoaZzZT0D0kb3f2GbNszkk67+9PZf5Sj3f0/22S2NZL+UfQy3tlqRZ39lxmXNFfSgyrwsUvMdZ9a8LgVcWafJulTd//c3c9I+rOkOQXM0fbc/R1Jp8/bPEfShuz2BvX9Y2m5KrO1BXc/5u7vZ7e/lnRumfFCH7vEXC1RRNnHS/pbv6+PqL3We3dJfzWzvWa2uOhhBjDO3Y9lt49LGlfkMAOouYx3K523zHjbPHb1LH/eKF6g+7Fb3H2qpLskLcmerrYl7/sZrJ2unQ5qGe9WGWCZ8X8p8rGrd/nzRhVR9qOSJvT7+ufZtrbg7kezzyclbVX7LUV94twKutnnkwXP8y/ttIz3QMuMqw0euyKXPy+i7HskXWdmvzCziyXNl7StgDl+xMxGZi+cyMxGSpqt9luKepukRdntRZJeL3CWH2iXZbyrLTOugh+7wpc/d/eWf0i6W32vyH8maVURM1SZ65eS/jf7OFD0bJI2qe9p3Xfqe23jYUn/JmmnpEOS3pI0po1m+29JH0rqUV+xOgua7Rb1PUXvkbQv+7i76McuMVdLHjd+XRYIghfogCAoOxAEZQeCoOxAEJQdCIKyA0FQdiCI/wfvpjt5Q0mdXQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Donc ici on utilise matplotlib pour affciher les images manuscrites \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "digit = train_images[0]\n",
        "plt.imshow(digit, cmap=plt.cm.binary)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79d40d78",
      "metadata": {
        "id": "79d40d78",
        "outputId": "84b78117-5bcd-4b83-8d08-b1908c22d4ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_labels[0]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "DeepLearningIAS1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}